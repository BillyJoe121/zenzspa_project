# üöÄ Sistema de Cola con Rate Limiting para Gemini API

## üìã √çndice

1. [Problema que Resuelve](#problema-que-resuelve)
2. [Arquitectura](#arquitectura)
3. [Configuraci√≥n](#configuraci√≥n)
4. [Uso](#uso)
5. [Monitoreo](#monitoreo)
6. [Escalabilidad](#escalabilidad)

---

## Problema que Resuelve

### L√≠mites de Gemini API (Plan Gratuito)
- **15 requests por minuto (RPM)**
- **1,500 requests por d√≠a (RPD)**
- **4 millones de tokens por d√≠a**

### Escenario Real
Con **40 usuarios** consumiendo sus **25 preguntas diarias**:
- Total: **1,000 requests/d√≠a** ‚úÖ Dentro del l√≠mite diario
- Problema: Si 20 usuarios escriben al mismo tiempo, se exceden los **15 RPM** ‚ùå

### Soluci√≥n
Cola inteligente con Celery que:
1. ‚úÖ **Respeta autom√°ticamente el l√≠mite de 15 RPM**
2. ‚úÖ **Procesa mensajes en orden** sin bloquear el servidor
3. ‚úÖ **Reintentos autom√°ticos** si hay errores temporales
4. ‚úÖ **Priorizaci√≥n** de usuarios premium sobre an√≥nimos (opcional)
5. ‚úÖ **No pierde mensajes** aunque el servidor se reinicie

---

## Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Usuario   ‚îÇ
‚îÇ  (Frontend) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /api/v1/bot/webhook/
       ‚îÇ {"message": "Hola"}
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Django (BotWebhookView)    ‚îÇ
‚îÇ                              ‚îÇ
‚îÇ  1. Validaciones r√°pidas     ‚îÇ
‚îÇ  2. Encolar tarea en Celery  ‚îÇ
‚îÇ  3. Devolver task_id         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Response: {"task_id": "abc123", "status": "queued"}
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Polling cada 2s
‚îÇ   Usuario   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  (Frontend) ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
       ‚îÇ GET /api/v1/bot/task-status/abc123/
       ‚ñº                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îê
‚îÇ   Django (BotTaskStatusView)      ‚îÇ
‚îÇ                                   ‚îÇ
‚îÇ  - status: 'pending' (en cola)    ‚îÇ
‚îÇ  - status: 'processing' (activo)  ‚îÇ
‚îÇ  - status: 'success' (listo)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚ñ≤
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Celery Worker (Background)       ‚îÇ
‚îÇ                                    ‚îÇ
‚îÇ  1. Verificar rate limit (15 RPM)  ‚îÇ
‚îÇ  2. Si OK: Llamar a Gemini         ‚îÇ
‚îÇ  3. Si l√≠mite alcanzado: Esperar   ‚îÇ
‚îÇ  4. Guardar log                    ‚îÇ
‚îÇ  5. Devolver respuesta             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Llamada controlada (max 15/min)
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gemini API   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Ventana Deslizante (Sliding Window)

El sistema usa una **ventana deslizante de 60 segundos** para controlar el rate limit:

```python
# Ejemplo: L√≠mite de 15 RPM
Minuto 1: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] ‚úÖ OK
         ‚Üë                                                    ‚Üë
       0:00                                                 0:55

Minuto 1+: [Request 16 llega en 1:05]
          ‚Üì
          Sistema verifica: ¬øCu√°ntas requests en los √∫ltimos 60s?
          - Desde 0:05 hasta 1:05 = Solo 14 requests
          - ‚úÖ Puede proceder (request 1 ya no cuenta)

Minuto 1+: [Request 17 llega en 1:06]
          ‚Üì
          Sistema verifica: ¬øCu√°ntas requests en los √∫ltimos 60s?
          - Desde 0:06 hasta 1:06 = 15 requests
          - ‚ùå L√≠mite alcanzado
          - ‚è≥ Esperar 2 segundos hasta que request 2 (en 0:08) salga de la ventana
```

Esto permite **uso continuo sin pausas artificiales**, a diferencia de un l√≠mite fijo por minuto.

---

## Configuraci√≥n

### 1. Instalar Redis (Broker de Celery)

Redis almacena la cola de tareas.

#### Windows
```bash
# Descargar Redis desde https://github.com/microsoftarchive/redis/releases
# O usar WSL:
wsl
sudo apt update
sudo apt install redis-server
redis-server
```

#### Linux/Mac
```bash
sudo apt install redis-server   # Ubuntu/Debian
brew install redis              # Mac
redis-server
```

Verificar que Redis est√° corriendo:
```bash
redis-cli ping
# Debe responder: PONG
```

### 2. Configurar Celery en Django

Ya est√° configurado en `studiozens/celery.py`. Solo necesitas iniciar el worker.

### 3. Variables de Entorno

Agregar en `.env`:
```bash
# Redis para Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Rate limit de Gemini (opcional, defaults a 15)
GEMINI_MAX_RPM=15
```

### 4. Iniciar Workers de Celery

Abrir **2 terminales**:

**Terminal 1: Worker para mensajes del bot**
```bash
# Windows
.\venv\Scripts\activate
celery -A studiozens worker --loglevel=info --pool=solo -Q bot_messages

# Linux/Mac
source venv/bin/activate
celery -A studiozens worker --loglevel=info -Q bot_messages
```

**Terminal 2: Worker para tareas de mantenimiento (opcional)**
```bash
celery -A studiozens worker --loglevel=info -Q celery
```

### 5. Iniciar Celery Beat (Tareas Programadas)

Para tareas cron como limpieza de logs, reportes diarios, etc.:

```bash
celery -A studiozens beat --loglevel=info
```

---

## Uso

### Modo 1: Sincr√≥nico (Actual - Sin Cola)

**Frontend:**
```javascript
const response = await fetch('/api/v1/bot/webhook/', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ message: 'Hola' })
});

const data = await response.json();
console.log(data.reply); // "¬°Hola! ¬øEn qu√© puedo ayudarte?"
```

**Pros:**
- ‚úÖ M√°s simple
- ‚úÖ No requiere polling

**Contras:**
- ‚ùå Puede exceder 15 RPM de Gemini si hay concurrencia
- ‚ùå Usuario espera bloqueado 5-20 segundos
- ‚ùå Si Gemini est√° lento, el request puede timeout

---

### Modo 2: As√≠ncrono con Cola (Recomendado)

#### Opci√≥n A: Modificar el Webhook para Usar Cola

Cambiar `BotWebhookView.post()` para encolar en lugar de procesar sincr√≥nicamente:

```python
# En bot/views/webhook.py - BotWebhookView.post()

# Despu√©s de validaciones...

# Encolar tarea en Celery
from .tasks import process_bot_message_async

task = process_bot_message_async.apply_async(
    kwargs={
        'user_id': user.id if user else None,
        'anonymous_user_id': anon_user.id if anon_user else None,
        'message': user_message,
        'client_ip': client_ip,
        'conversation_history': conversation_history
    },
    queue='bot_messages',  # Cola espec√≠fica para mensajes
    priority=5 if user else 3  # Prioridad: usuarios > an√≥nimos
)

return Response({
    'task_id': task.id,
    'status': 'queued',
    'message': 'Tu mensaje est√° siendo procesado...'
}, status=status.HTTP_202_ACCEPTED)
```

**Frontend con Polling:**
```javascript
async function sendMessage(message) {
  // 1. Enviar mensaje y obtener task_id
  const response = await fetch('/api/v1/bot/webhook/', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message })
  });

  const { task_id, status } = await response.json();

  if (status === 'queued') {
    // 2. Hacer polling hasta que est√© listo
    const reply = await pollTaskStatus(task_id);
    return reply;
  }
}

async function pollTaskStatus(taskId) {
  let attempts = 0;
  const maxAttempts = 30; // 30 * 2s = 60s timeout

  while (attempts < maxAttempts) {
    const response = await fetch(`/api/v1/bot/task-status/${taskId}/`);
    const data = await response.json();

    if (data.status === 'success') {
      return data.reply;
    } else if (data.status === 'failure') {
      throw new Error(data.error);
    } else {
      // A√∫n procesando, mostrar indicador de carga
      console.log('Procesando...');
      await new Promise(resolve => setTimeout(resolve, 2000)); // Esperar 2s
      attempts++;
    }
  }

  throw new Error('Timeout esperando respuesta del bot');
}
```

#### Opci√≥n B: Endpoint Dedicado para Cola (Sin Modificar Webhook Actual)

Crear un nuevo endpoint `/api/v1/bot/webhook-async/` que use la cola, y mantener el actual sincr√≥nico para compatibilidad:

```python
class BotWebhookAsyncView(APIView):
    """Versi√≥n as√≠ncrona del webhook que usa cola"""
    permission_classes = [AllowAny]

    def post(self, request):
        # ... validaciones ...

        task = process_bot_message_async.apply_async(...)

        return Response({
            'task_id': task.id,
            'status': 'queued'
        }, status=status.HTTP_202_ACCEPTED)
```

---

## Monitoreo

### 1. Flower (Dashboard de Celery)

Instalar:
```bash
pip install flower
```

Iniciar:
```bash
celery -A studiozens flower --port=5555
```

Abrir: http://localhost:5555

**Ver√°s:**
- üìä Tareas en cola, procesando, completadas
- ‚è±Ô∏è Tiempos de ejecuci√≥n
- ‚ùå Tareas fallidas con detalles
- üîÑ Workers activos
- üìà Gr√°ficos de throughput

### 2. Logs en Tiempo Real

Ver logs del worker:
```bash
tail -f logs/celery_worker.log
```

Buscar rate limits:
```bash
grep "Rate limit alcanzado" logs/celery_worker.log
```

### 3. M√©tricas en Redis

Ver tareas pendientes:
```bash
redis-cli
> LLEN celery  # Tareas en cola default
> LLEN bot_messages  # Tareas en cola de bot
```

### 4. Django Admin

Las tareas procesadas se guardan en `BotConversationLog` con metadatos:

```python
# Admin ‚Üí Bot ‚Üí Logs de Conversaci√≥n
log.response_meta = {
    'task_id': 'abc123',
    'processing_time_seconds': 2.5,
    'gemini_latency_ms': 1800,
    'source': 'gemini-rag',
    'tokens': 250
}
```

---

## Escalabilidad

### Escenario 1: Tr√°fico Bajo (< 10 usuarios concurrentes)

**Configuraci√≥n:**
- 1 worker de Celery
- Redis en mismo servidor

```bash
celery -A studiozens worker --loglevel=info --concurrency=4
```

### Escenario 2: Tr√°fico Medio (10-50 usuarios concurrentes)

**Configuraci√≥n:**
- 2-3 workers de Celery (diferentes procesos)
- Redis dedicado
- Priorizaci√≥n de colas

**Worker 1: Alta prioridad (usuarios registrados)**
```bash
celery -A studiozens worker -Q bot_messages -n worker1@%h --concurrency=8
```

**Worker 2: Baja prioridad (an√≥nimos)**
```bash
celery -A studiozens worker -Q bot_messages_low -n worker2@%h --concurrency=4
```

### Escenario 3: Tr√°fico Alto (50+ usuarios concurrentes)

**Configuraci√≥n:**
- 5+ workers distribuidos en m√∫ltiples servidores
- Redis Cluster
- RabbitMQ en lugar de Redis (m√°s robusto)
- Monitoreo con Prometheus + Grafana

```bash
# Servidor 1
celery -A studiozens worker -Q bot_messages --concurrency=16 -n worker1@server1

# Servidor 2
celery -A studiozens worker -Q bot_messages --concurrency=16 -n worker2@server2
```

### Ajustar Concurrency

Por defecto, Celery usa `concurrency = CPU_CORES`. Para tareas IO-bound (como llamar a Gemini), puedes aumentarlo:

```bash
# Si tienes 4 cores, puedes usar 16 workers concurrentes
celery -A studiozens worker --concurrency=16
```

### L√≠mite de Throughput

Con **15 RPM de Gemini**:
- **M√°ximo throughput:** 15 mensajes/minuto = 900 mensajes/hora
- Con 40 usuarios usando 25 mensajes/d√≠a = **1,000 mensajes/d√≠a** ‚úÖ OK
- Pico m√°ximo: Si todos escriben al mismo tiempo, habr√° **delay de cola**

**Ejemplo:** Si llegan 30 mensajes en 1 minuto:
- Primeros 15: Procesados inmediatamente
- Siguientes 15: Esperan en cola ~60 segundos

---

## Priorizaci√≥n de Usuarios

Para dar mejor experiencia a usuarios premium:

```python
# En bot/tasks.py

@shared_task(bind=True, max_retries=5)
def process_bot_message_async(self, user_id=None, ...):
    # ... c√≥digo actual ...
    pass

# Al encolar en views.py:
task = process_bot_message_async.apply_async(
    kwargs={...},
    queue='bot_messages_high' if user and user.is_premium else 'bot_messages_low',
    priority=10 if user and user.is_premium else 5
)
```

Iniciar workers dedicados:
```bash
# Worker para usuarios premium (m√°s workers)
celery -A studiozens worker -Q bot_messages_high --concurrency=10

# Worker para an√≥nimos (menos workers)
celery -A studiozens worker -Q bot_messages_low --concurrency=4
```

---

## Comandos √ötiles

### Ver Estado de Workers
```bash
celery -A studiozens inspect active
celery -A studiozens inspect stats
```

### Purgar Cola
```bash
celery -A studiozens purge
```

### Reiniciar Workers Sin Perder Tareas
```bash
celery -A studiozens control shutdown  # Graceful shutdown
# Luego reiniciar con el comando worker
```

### Ver Tareas Registradas
```bash
celery -A studiozens inspect registered
```

---

## Troubleshooting

### Worker No Procesa Tareas

**Verificar:**
1. Redis est√° corriendo: `redis-cli ping`
2. Worker est√° activo: `celery -A studiozens inspect active`
3. Las colas coinciden: Tarea usa `queue='bot_messages'` y worker est√° escuchando esa cola

### Rate Limit No Se Respeta

**Verificar:**
1. Redis tiene la clave: `redis-cli GET gemini_api_rate_limit`
2. Solo hay un worker por servidor (no m√∫ltiples compitiendo)
3. El sistema de ventana deslizante est√° funcionando

### Tareas Fallan con "User Not Found"

**Causa:** La sesi√≥n del usuario expir√≥ entre el momento de encolar y procesar.

**Soluci√≥n:** Agregar manejo de errores en la tarea:
```python
if user_id:
    try:
        user = User.objects.get(id=user_id)
    except User.DoesNotExist:
        logger.error("Usuario no encontrado: %s", user_id)
        return {'error': 'Sesi√≥n expirada'}
```

---

## Roadmap Futuro

Posibles mejoras:

1. **WebSockets en lugar de Polling**
   - Usar Django Channels
   - Notificar al usuario cuando la respuesta est√© lista
   - Mejor UX (sin polling)

2. **Rate Limit Distribuido**
   - Usar Redis Lua scripts para atomicidad
   - Soportar m√∫ltiples workers en diferentes servidores

3. **An√°lisis Predictivo**
   - Predecir carga futura basado en patrones hist√≥ricos
   - Auto-escalar workers seg√∫n demanda

4. **Fallback a Otro Modelo**
   - Si Gemini est√° saturado, usar Claude/GPT como backup
   - Degradaci√≥n graciosa

---

## Conclusi√≥n

Este sistema te permite:

‚úÖ **Respetar el l√≠mite de 15 RPM de Gemini** autom√°ticamente
‚úÖ **Manejar 40+ usuarios concurrentes** sin errores
‚úÖ **No perder mensajes** aunque el servidor se reinicie
‚úÖ **Escalar horizontalmente** agregando m√°s workers
‚úÖ **Monitorear** toda la actividad con Flower y logs
‚úÖ **Priorizar** usuarios premium sobre an√≥nimos

El costo de esto es:
- ‚öôÔ∏è Configurar y mantener Redis + Celery
- üîÑ Cambiar frontend para hacer polling
- üìä Monitorear workers

**Recomendaci√≥n:** Si tu tr√°fico actual es bajo (< 10 usuarios concurrentes), el modo sincr√≥nico actual es suficiente. Implementa la cola cuando notes que empiezas a tener muchos rate limit errors de Gemini.
